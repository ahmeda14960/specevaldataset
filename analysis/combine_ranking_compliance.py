#!/usr/bin/env python3
"""Combine and visualize ranking summaries from multiple ranker models.

This script reads multiple 'ranking_summary.csv' files, each generated by
'analyze_rankings.py' for a specific ranker model. It combines these summaries
to compare how different rankers score and rank the candidate models.

Output includes a combined CSV and visualizations comparing ranker performance.
"""

import pandas as pd
import yaml
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse


def load_config(config_path):
    """Load configuration from YAML file."""
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def extract_ranker_from_path(path_str):
    """Extract ranker name from file path."""
    # Assuming path structure: analysis/rankings_analysis/RANKER_{ranker_name}/.../ranking_summary.csv
    # Or just the ranker name if provided directly
    path = Path(path_str)
    # Try extracting from parent directory name first
    try:
        # Go up two levels from the file (ranking_summary.csv -> evaluator -> ranker)
        ranker_part = path.parent.parent.name
        # Check if it follows the potential pattern, otherwise return the directory name itself
        # This is heuristic, might need adjustment based on actual structure
        # if ranker_part.startswith("RANKER_"): # Simple check, adjust if needed
        #     return ranker_part.replace("RANKER_", "").lower()
        return ranker_part  # Return the directory name as the ranker identifier
    except IndexError:
        # Fallback if path is too short, return filename stem or unknown
        return path.stem if path.stem != "ranking_summary" else "unknown_ranker"


def combine_ranking_data(config):
    """Combine ranking summary data from multiple ranker CSV files.

    Args:
        config (dict): Configuration dictionary containing paths and settings.
    """
    output_dir = Path(config.get("output_dir", "analysis/multi_ranker_comparison"))
    csv_paths = config.get("ranking_summary_csvs", [])
    score_metric = config.get("score_metric", "wins_minus_losses")  # 'wins' or 'wins_minus_losses'

    if not csv_paths:
        print("No ranking_summary_csvs provided in config file")
        return

    output_dir.mkdir(parents=True, exist_ok=True)

    ranker_data = {}
    all_models = set()

    # Load each CSV file
    for csv_path in csv_paths:
        path_obj = Path(csv_path)
        if not path_obj.exists():
            print(f"Warning: CSV file not found: {csv_path}")
            continue

        ranker = extract_ranker_from_path(csv_path)
        if ranker in ranker_data:
            print(f"Warning: Duplicate ranker '{ranker}' detected from path {csv_path}. Skipping.")
            continue

        try:
            df = pd.read_csv(csv_path)
            # Ensure the first column (model names) is used as index if needed, handle unnamed column
            if df.columns[0].startswith("Unnamed"):
                df = df.rename(columns={df.columns[0]: "model"})
            df = df.set_index("model")

            # Calculate score based on config
            if score_metric == "wins":
                if "wins" not in df.columns:
                    print(
                        f"Warning: 'wins' column not found in {csv_path} for ranker {ranker}. Skipping."
                    )
                    continue
                df["score"] = df["wins"]
            elif score_metric == "wins_minus_losses":
                if "wins" not in df.columns or "losses" not in df.columns:
                    print(
                        f"Warning: 'wins' or 'losses' column not found in {csv_path} for ranker {ranker}. Skipping."
                    )
                    continue
                df["score"] = df["wins"] - df["losses"]
            else:
                print(f"Warning: Invalid score_metric '{score_metric}'. Using 'wins'.")
                if "wins" not in df.columns:
                    print(
                        f"Warning: 'wins' column not found in {csv_path} for ranker {ranker}. Skipping."
                    )
                    continue
                df["score"] = df["wins"]

            # Store relevant data (scores)
            ranker_data[ranker] = df["score"]
            all_models.update(df.index.tolist())

        except Exception as e:
            print(f"Error processing {csv_path}: {e}")
            continue

    if not ranker_data:
        print("No valid ranker data loaded. Exiting.")
        return

    # Create combined dataframe
    combined_df = pd.DataFrame(index=list(all_models))
    for ranker, scores in ranker_data.items():
        combined_df[f"{ranker}_score"] = scores

    # Calculate average score and sort
    score_cols = [col for col in combined_df.columns if col.endswith("_score")]
    if score_cols:
        combined_df["avg_score"] = combined_df[score_cols].mean(axis=1)
        combined_df = combined_df.sort_values("avg_score", ascending=False)

    # Add ranks per ranker
    for ranker in ranker_data.keys():
        combined_df[f"{ranker}_rank"] = combined_df[f"{ranker}_score"].rank(
            method="min", ascending=False
        )

    # Reset index to make model a column
    combined_df = combined_df.reset_index().rename(columns={"index": "model"})

    # Save to CSV
    output_csv = output_dir / "combined_ranker_comparison.csv"
    combined_df.to_csv(output_csv, index=False, float_format="%.2f")
    print(f"Combined ranker comparison saved to {output_csv}")

    # Create visualizations
    create_visualizations(combined_df, output_dir, score_metric)

    return combined_df


def create_visualizations(combined_df, output_dir, score_metric):
    """Create visualizations of combined ranker data."""
    score_cols = [
        col for col in combined_df.columns if col.endswith("_score") and col != "avg_score"
    ]
    rank_cols = [col for col in combined_df.columns if col.endswith("_rank")]

    num_rankers = len(score_cols)
    num_models = len(combined_df)

    if num_rankers < 1:
        print("No ranker scores found for visualization.")
        return

    # Use model name as index for plotting convenience
    plot_df = combined_df.set_index("model")

    # 1. Heatmap of scores
    plt.figure(figsize=(max(10, num_rankers * 1.5), max(8, num_models * 0.5)))
    score_heatmap_df = plot_df[score_cols].copy()
    score_heatmap_df.columns = [
        col.replace("_score", "") for col in score_heatmap_df.columns
    ]  # Cleaner labels
    # Sort the original plot_df by avg_score and use its index to reorder the heatmap df
    sorted_index = plot_df.sort_values(by="avg_score", ascending=False).index
    sns.heatmap(
        score_heatmap_df.loc[sorted_index], annot=True, cmap="viridis", fmt=".0f", linewidths=0.5
    )
    plt.title(f"Model Scores by Ranker (Metric: {score_metric})")
    plt.xlabel("Ranker")
    plt.ylabel("Candidate Model")
    plt.xticks(rotation=45, ha="right")
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(output_dir / "ranker_scores_heatmap.png", dpi=300)
    plt.close()

    # 2. Bar chart of average scores
    if "avg_score" in plot_df.columns:
        plt.figure(figsize=(max(10, num_models * 0.6), 6))
        plot_df["avg_score"].sort_values(ascending=False).plot(kind="bar", color="teal")
        plt.axhline(
            y=plot_df["avg_score"].mean(),
            color="r",
            linestyle="--",
            label=f'Overall Avg ({plot_df["avg_score"].mean():.1f})',
        )
        plt.title(f"Average Model Score Across All Rankers (Metric: {score_metric})")
        plt.ylabel(f"Average Score ({score_metric})")
        plt.xlabel("Candidate Model")
        plt.xticks(rotation=45, ha="right")
        plt.legend()
        plt.tight_layout()
        plt.savefig(output_dir / "average_model_score_across_rankers.png", dpi=300)
        plt.close()

    # 3. Heatmap of ranks
    plt.figure(figsize=(max(10, num_rankers * 1.5), max(8, num_models * 0.5)))
    rank_heatmap_df = plot_df[rank_cols].copy()
    rank_heatmap_df.columns = [
        col.replace("_rank", "") for col in rank_heatmap_df.columns
    ]  # Cleaner labels
    # Sort by average score to maintain consistency
    # Use the same sorted index derived from avg_score
    sorted_index = plot_df.sort_values(by="avg_score", ascending=False).index
    sns.heatmap(
        rank_heatmap_df.loc[sorted_index],
        annot=True,
        cmap="viridis_r",
        fmt=".0f",
        linewidths=0.5,
        cbar_kws={"label": "Rank (lower is better)"},
    )
    plt.title("Model Ranks by Ranker (Lower is Better)")
    plt.xlabel("Ranker")
    plt.ylabel("Candidate Model")
    plt.xticks(rotation=45, ha="right")
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(output_dir / "ranker_ranks_heatmap.png", dpi=300)
    plt.close()

    print(f"Visualizations saved to {output_dir}")


def plot_agreement_matrix(
    agreement_matrix: pd.DataFrame, output_path: Path, title: str = "Ranker Agreement Matrix"
):
    """Generate a heatmap of agreement between ranker models.

    Args:
        agreement_matrix (pd.DataFrame): Matrix of agreement scores between rankers
        output_path (Path): Path to save the output plot
        title (str): Title for the plot
    """
    if agreement_matrix.empty or agreement_matrix.shape[0] < 2:
        print(f"Skipping plot '{title}': Not enough data.")
        return


def main():
    """Run the main ranking combination pipeline."""
    parser = argparse.ArgumentParser(
        description="Combine ranking summary data from multiple rankers."
    )
    parser.add_argument(
        "--config",
        default="analysis/combine_rankings_config.yaml",
        help="Path to configuration file listing ranking_summary.csv files and output directory.",
    )
    args = parser.parse_args()

    if not Path(args.config).is_file():
        print(f"Error: Configuration file not found at {args.config}")
        # Create a template config file if it doesn't exist
        template_config = {
            "ranking_summary_csvs": [
                "analysis/rankings_analysis/ranker_model_1/evaluator_model/ranking_summary.csv",
                "analysis/rankings_analysis/ranker_model_2/evaluator_model/ranking_summary.csv",
                # Add more paths here
            ],
            "score_metric": "wins_minus_losses",  # Options: 'wins', 'wins_minus_losses'
            "output_dir": "analysis/multi_ranker_comparison",
        }
        try:
            with open(args.config, "w") as f:
                yaml.dump(template_config, f, default_flow_style=False, sort_keys=False)
            print(
                f"Created template configuration file at {args.config}. Please edit it with your CSV paths."
            )
        except Exception as e:
            print(f"Failed to create template config file: {e}")
        return

    try:
        config = load_config(args.config)
        combine_ranking_data(config)
        print("Analysis complete.")
    except Exception as e:
        print(f"An error occurred: {e}")


if __name__ == "__main__":
    main()
