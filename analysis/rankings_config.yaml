# Configuration file for analyze_rankings.py

# Name of the ranker model folder under data/rankings
# The ranker is the model that performed the A/B comparison
#ranker: ranker_gpt-4.1-mini-2025-04-14
#ranker: ranker_claude-3-5-haiku-20241022
ranker: ranker_gemini-2.0-flash-001
# Name of the evaluator folder under data/rankings/<ranker>
# The evaluator is the model that generated the prompts/inputs for ranking
evaluator: gpt-4.1-2025-04-14

# List of candidate models (suffixes of the VS folders under the evaluator)
models:
  - claude-3-7-sonnet-20250219
  - Qwen-Qwen2.5-72B-Instruct-Turbo
  - meta-llama-Meta-Llama-3.1-405B-Instruct-Lite-Pro
  - mistralai-Mistral-7B-Instruct-v0.3
  - gemini-2.0-flash-001
  - deepseek-ai-DeepSeek-R1
  - gpt-4.1-2025-04-14

# If true, correct for position bias by combining A vs B and B vs A
type: all_pairs
all_pairs: true

# Input/output paths (relative to project root)
rankings_dir: data/rankings
output_dir: analysis/rankings_analysis
