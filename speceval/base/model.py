"""Base model classes for the SpecEval framework."""
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from pathlib import Path
from speceval.base.statement import Statement


class BaseModel(ABC):
    """Base abstract class for all models."""

    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """Generate a response based on the prompt."""
        pass


class CandidateModel(BaseModel):
    """Model being evaluated for compliance with specifications."""

    @abstractmethod
    def get_info(self) -> Dict[str, Any]:
        """Return metadata about the model."""
        pass

    def supports_caching(self) -> bool:
        """
        Return whether this model implementation supports caching.

        Default is False, override in subclasses to enable caching.
        """
        return False


class EvaluatorModel(BaseModel):
    """Model used for generating inputs that might challenge compliance."""

    @abstractmethod
    def generate_challenging_input(
        self, statement: "Statement", previous_inputs: Optional[List[str]] = None, **kwargs
    ) -> str:
        """
        Generate an input designed to challenge compliance with a statement.

        Args:
            statement: The statement to generate a challenging input for.
            previous_inputs: Optional list of previously generated inputs for this statement.
                             Used to avoid repetition and ensure diversity in generations.
            **kwargs: Additional arguments for generation.

        Returns:
            A string containing the generated challenging input.
        """
        pass

    @abstractmethod
    def get_info(self) -> Dict[str, Any]:
        """Return metadata about the model, including at least the model name."""
        pass

    def supports_caching(self) -> bool:
        """
        Return whether this model implementation supports caching.

        Default is False, override in subclasses to enable caching.
        """
        return False


class JudgeModel(BaseModel):
    """Model used for evaluating compliance of outputs with specifications."""

    @abstractmethod
    def evaluate_compliance(
        self, statement: "Statement", input_text: str, output_text: str, **kwargs
    ) -> Dict[str, Any]:
        """
        Evaluate compliance of an output with a statement.

        Returns:
            Dict with at least:
                - 'compliant': bool
                - 'confidence': float (0-1)
                - 'explanation': str
        """
        pass

    @abstractmethod
    def get_info(self) -> Dict[str, Any]:
        """Return metadata about the model."""
        pass

    def supports_caching(self) -> bool:
        """
        Return whether this model implementation supports caching.

        Default is False, override in subclasses to enable caching.
        """
        return False


class RankingModel(BaseModel):
    """Model used for ranking responses based on specifications."""

    @abstractmethod
    def rank_responses(
        self, statement: Statement, input_text: str, output_a: str, output_b: str, **kwargs
    ) -> int:
        """
        Compare two model outputs (A and B) for a given input and statement context.

        Args:
            statement: The policy statement context.
            input_text: The input prompt given to both models.
            output_a: The response generated by model A.
            output_b: The response generated by model B.
            **kwargs: Additional arguments for ranking.

        Returns:
            1 if output_a is preferred over output_b based on the statement.
           -1 if output_b is preferred over output_a based on the statement.
            0 if they are considered equal in quality/compliance/alignment or if the statement is not applicable.
        """
        pass

    @abstractmethod
    def get_info(self) -> Dict[str, Any]:
        """Return metadata about the ranking model."""
        pass

    def supports_caching(self) -> bool:
        """
        Return whether this model implementation supports caching.

        Default is False, override in subclasses to enable caching.
        """
        return False


class BatchedModel(BaseModel):
    """Abstract base class for models supporting batched inference."""

    @abstractmethod
    def generate_batch(
        self,
        prompts_data: List[Dict[str, Any]],
        batch_output_dir: Path,
        batch_size: Optional[int] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Submit a batch of prompts for asynchronous generation.

        Args:
            prompts_data: A list of dictionaries, each containing prompt details
                          (e.g., {'input_text': ..., 'statement_id': ..., 'original_index': ...}).
            batch_output_dir: The specific run's output directory for intermediate files.
            batch_size: Optional batch size hint (the implementation might handle its own batching).
            **kwargs: Additional arguments for the batch generation request.

        Returns:
            A dictionary containing metadata about the submitted batch, typically including:
            - 'batch_id': A unique identifier for this batch job.
            - 'status': The initial status (e.g., "processing", "submitted").
            - Potentially other relevant info like estimated cost, number of prompts, etc.
        """
        pass

    @abstractmethod
    def check_batch_progress(self, batch_metadata: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """
        Check the progress of a previously submitted batch job.

        Args:
            batch_metadata: The metadata dictionary returned by generate_batch.
            **kwargs: Additional arguments for checking progress.

        Returns:
            An updated status dictionary, typically including:
            - 'batch_id': The batch identifier.
            - 'status': The current status ("processing", "completed", "failed", etc.).
            - Optionally: 'progress_percent', 'error_message', 'results_location' if completed.
        """
        pass

    @abstractmethod
    def get_batch_results(
        self, batch_metadata: Dict[str, Any], **kwargs
    ) -> Dict[str, Optional[str]]:
        """
        Retrieve the results for a completed batch job.

        Args:
            batch_metadata: The metadata dictionary returned by generate_batch (ideally with status "completed").
            **kwargs: Additional arguments for retrieving results.

        Returns:
            A dictionary mapping the original custom_id provided for each request
            to its corresponding result string or an error string/None if failed.
            Should raise an error if the batch is not complete or failed.
        """
        pass

    @abstractmethod
    def get_info(self) -> Dict[str, Any]:
        """Return metadata about the batched model."""
        pass

    # Batch models generally don't fit the simple request-response caching model well.
    # Caching might be handled internally by the batch provider or require a different mechanism.
    def supports_caching(self) -> bool:
        """Indicate whether standard caching is supported (typically False for batch)."""
        return False


# New Async model interfaces
class AsyncModel(ABC):
    """Base abstract class for asynchronous model operations."""

    @abstractmethod
    async def generate_async(self, prompt: str, **kwargs) -> str:
        """Asynchronously generate a response based on the prompt."""
        pass


class AsyncEvaluatorModel(EvaluatorModel, AsyncModel):
    """Asynchronous evaluator model for generating challenging inputs."""

    @abstractmethod
    async def generate_challenging_input_async(
        self,
        statement: "Statement",
        previous_inputs: Optional[List[str]] = None,
        **kwargs,
    ) -> str:
        """Asynchronously generate a challenging input for a statement."""
        pass


class AsyncJudgeModel(JudgeModel, AsyncModel):
    """Asynchronous judge model for evaluating compliance."""

    @abstractmethod
    async def evaluate_compliance_async(
        self,
        statement: "Statement",
        input_text: str,
        output_text: str,
        **kwargs,
    ) -> Dict[str, Any]:
        """Asynchronously evaluate compliance of an output with a statement."""
        pass
