#!/usr/bin/env python3
"""
Example script demonstrating how to use the SpecEval framework
to evaluate compliance with the OpenAI Model Spec using Anthropic models.

This script supports multi-level caching to improve performance and save API costs:
- Level "none": No caching (all API calls will be made each time)
- Level "inputs": Only cache the challenging inputs generated by the AutoBencher
- Level "generations": Cache both inputs and candidate model responses
- Level "all": Cache inputs, candidate model responses, and judge model evaluations
"""

import os
import argparse
import datetime
from pathlib import Path

from speceval import (
    JsonlParser,
    StandardPipeline,
)
from speceval.pipelines.standard_pipeline import CacheLevel
from speceval.orgs.anthropic import (
    Anthropic,
)
from speceval.utils.logging import setup_logging


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate Anthropic models for compliance with the OpenAI Model Spec",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Caching functionality:
  This script supports multi-level caching to improve performance and save API costs:
   - Level "none": No caching (all API calls will be made each time)
   - Level "inputs": Only cache the challenging inputs generated by the AutoBencher
   - Level "generations": Cache both inputs and candidate model responses
   - Level "all": Cache inputs, candidate model responses, and judge model evaluations

  Cached data is stored in subdirectories under the cache-dir:
   - Inputs: {cache-dir}/autobencher_outputs/
   - Generations: {cache-dir}/generations/
   - Judgements: {cache-dir}/judgements/
""",
    )
    parser.add_argument(
        "--spec-path",
        type=str,
        default="data/specs/openai/jsonl/openai.jsonl",
        help="Path to the OpenAI Model Spec JSONL file",
    )
    parser.add_argument(
        "--num-test-cases", type=int, default=5, help="Number of test cases to generate"
    )
    parser.add_argument(
        "--num-inputs-per-statement",
        type=int,
        default=1,
        help="Number of inputs to generate per statement",
    )
    parser.add_argument(
        "--test-all-statements", action="store_true", help="Test all statements instead of sampling"
    )
    parser.add_argument(
        "--output-path",
        type=str,
        default="compliance_report.html",
        help="Path to save the HTML report",
    )
    parser.add_argument(
        "--anthropic-api-key",
        type=str,
        default=None,
        help="Anthropic API key (alternatively, set ANTHROPIC_API_KEY env var)",
    )
    parser.add_argument(
        "--openai-api-key",
        type=str,
        default=None,
        help="OpenAI API key for evaluator/judge (alternatively, set OPENAI_API_KEY env var)",
    )
    parser.add_argument(
        "--cache-level",
        type=str,
        choices=["none", "inputs", "generations", "all"],
        default="all",
        help="Level of caching to use (none, inputs, generations, all)",
    )
    parser.add_argument(
        "--cache-dir",
        type=str,
        default="data",
        help="Base directory to store cached data",
    )
    parser.add_argument(
        "--reports-dir",
        type=str,
        default="data/reports",
        help="Directory to store evaluation reports",
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    parser.add_argument(
        "--model",
        type=str,
        choices=["sonnet", "haiku", "opus"],
        default="sonnet",
        help="Which Claude model to evaluate",
    )
    parser.add_argument(
        "--evaluator-provider",
        type=str,
        choices=["anthropic", "openai"],
        default="anthropic",
        help="Provider for the evaluator model",
    )
    parser.add_argument(
        "--candidate-model",
        type=str,
        default="claude-3-7-sonnet-20250219",
        help="Model name for the candidate",
    )
    parser.add_argument(
        "--evaluator-model",
        type=str,
        default="claude-3-7-sonnet-20250219",
        help="Model name for the evaluator",
    )
    parser.add_argument(
        "--judge-provider",
        type=str,
        choices=["anthropic", "openai", "google"],
        default="anthropic",
        help="Provider for the judge model",
    )
    parser.add_argument(
        "--judge-model",
        type=str,
        default="claude-3-7-sonnet-20250219",
        help="Model name for the judge",
    )
    parser.add_argument(
        "--pregenerated-inputs-dir",
        type=str,
        default=None,
        help="Optional path to a directory containing pre-generated JSON inputs for statements (filename stem must match statement ID)",
    )

    args = parser.parse_args()

    # Configure logging
    logger = setup_logging(args.verbose, folder_name="eval_anthropic_models")

    # If API key is provided, use it; otherwise use environment variable
    anthropic_api_key = args.anthropic_api_key or os.environ.get("ANTHROPIC_API_KEY")
    if not anthropic_api_key:
        raise ValueError(
            "Anthropic API key is required. Either provide --anthropic-api-key or set ANTHROPIC_API_KEY env var."
        )

    # Check if we need an OpenAI API key
    openai_api_key = args.openai_api_key or os.environ.get("OPENAI_API_KEY")
    if (
        args.evaluator_provider == "openai" or args.judge_provider == "openai"
    ) and not openai_api_key:
        raise ValueError(
            "OpenAI API key is required when using OpenAI models. Either provide --openai-api-key or set OPENAI_API_KEY env var."
        )

    # Create the specification parser
    parser = JsonlParser()

    # Load the specification
    spec_path = Path(args.spec_path)
    if not spec_path.exists():
        raise FileNotFoundError(f"Specification file not found: {spec_path}")

    spec = parser.from_file(spec_path)
    logger.info(f"Loaded specification with {len(spec.statements)} statements")

    # Display the first five statements if verbose mode is enabled
    if args.verbose and spec.statements:
        logger.debug("First five statements from the specification:")
        for i, statement in enumerate(spec.statements[:5]):
            logger.debug(f"{i+1}. {statement}")

    # Create the appropriate organization based on the model choice
    organization = Anthropic(
        api_key=anthropic_api_key,
        candidate_model_name=args.candidate_model,
        evaluator_provider=args.evaluator_provider,
        judge_provider=args.judge_provider,
        evaluator_model_name=args.evaluator_model,
        judge_model_name=args.judge_model,
        openai_api_key=openai_api_key,
    )

    # Map string cache level to enum
    cache_level_map = {
        "none": CacheLevel.NONE,
        "inputs": CacheLevel.INPUTS,
        "generations": CacheLevel.GENERATIONS,
        "all": CacheLevel.ALL,
    }
    cache_level = cache_level_map[args.cache_level]

    # Create the pipeline
    pipeline = StandardPipeline(
        specification=spec,
        organization=organization,
        num_test_cases=args.num_test_cases,
        statements_to_test=spec.statements if args.test_all_statements else None,
        num_inputs_per_statement=args.num_inputs_per_statement,
        cache_level=cache_level,
        cache_dir=args.cache_dir,
        verbose=args.verbose,
        pregenerated_inputs_dir=args.pregenerated_inputs_dir,
    )

    # Run the evaluation
    logger.info(
        f"Running evaluation for {organization.get_info()['name']} with {args.num_test_cases} test cases..."
    )
    if args.test_all_statements:
        logger.info(
            f"Testing all {len(spec.statements)} statements with {args.num_inputs_per_statement} inputs per statement"
        )

    # Log caching configuration
    if cache_level == CacheLevel.NONE:
        logger.info("Caching is disabled")
    elif cache_level == CacheLevel.INPUTS:
        logger.info(f"Caching inputs only in {args.cache_dir}/autobencher_outputs")
    elif cache_level == CacheLevel.GENERATIONS:
        logger.info(f"Caching inputs and generations in {args.cache_dir}")
    else:  # CacheLevel.ALL
        logger.info(f"Caching inputs, generations, and judgements in {args.cache_dir}")

    results = pipeline.run()

    # Print a summary
    summary = results.get_summary()
    logger.info(f"\nEvaluation Summary for {organization.get_info()['name']}:")
    logger.info(f"Total test cases: {summary['total_cases']}")
    logger.info(f"Compliant cases: {summary['compliant_cases']}")
    logger.info(f"Compliance rate: {summary['compliance_rate'] * 100:.2f}%")

    # Print compliance by statement type
    logger.info("\nCompliance by Statement Type:")
    for type_name, stats in summary["by_type"].items():
        logger.info(
            f"  {type_name}: {stats['compliance_rate'] * 100:.2f}% ({stats['compliant']}/{stats['total']})"
        )

    # Print compliance by authority level
    logger.info("\nCompliance by Authority Level:")
    for auth_name, stats in summary["by_authority"].items():
        logger.info(
            f"  {auth_name}: {stats['compliance_rate'] * 100:.2f}% ({stats['compliant']}/{stats['total']})"
        )

    # Generate HTML report
    output_path = Path(args.output_path)
    results.to_html(output_path)
    logger.info(f"\nGenerated HTML report: {output_path.absolute()}")

    # Also save JSON for further analysis
    json_path = output_path.with_suffix(".json")
    results.to_json(json_path)
    logger.info(f"Generated JSON report: {json_path.absolute()}")

    # Save reports to the specified directory with timestamp
    reports_dir = Path(args.reports_dir)
    if not reports_dir.exists():
        reports_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Created reports directory: {reports_dir.absolute()}")

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    # Get organization info
    org_info = organization.get_info()

    # Sanitize model names and create more descriptive filename
    candidate_model_name = org_info["candidate_model"].replace("/", "-")

    # Extract provider information from the model strings
    evaluator_provider, evaluator_model_name = org_info["evaluator_model"].split("/", 1)
    judge_provider, judge_model_name = org_info["judge_model"].split("/", 1)

    # Sanitize model names
    evaluator_model_name = evaluator_model_name.replace("/", "-")
    judge_model_name = judge_model_name.replace("/", "-")

    report_filename = (
        f"{candidate_model_name}_eval_{evaluator_provider}_{evaluator_model_name}_"
        f"judge_{judge_provider}_{judge_model_name}_{timestamp}.json"
    )
    report_path = reports_dir / report_filename

    results.to_json(report_path)
    logger.info(f"Saved detailed evaluation report to: {report_path.absolute()}")


if __name__ == "__main__":
    main()
