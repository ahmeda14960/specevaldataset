#!/usr/bin/env python3
"""
Example script demonstrating how to use the SpecEval framework
to evaluate compliance with the OpenAI Model Spec using mixed provider models.

This script allows evaluating models from any combination of providers:
- Candidate model: OpenAI, Anthropic, or Together
- Evaluator model: OpenAI or Anthropic
- Judge model: OpenAI or Anthropic

This provides flexibility to test different model combinations, such as:
- Claude as the candidate with GPT-4 as the evaluator and judge
- GPT-4 as the candidate with Claude as the evaluator and judge
- Llama 3 as the candidate with GPT-4 as the evaluator and judge
"""

import os
import argparse
import datetime
from pathlib import Path

from speceval import (
    MarkdownParser,
    StandardPipeline,
)
from speceval.pipelines.standard_pipeline import CacheLevel
from speceval.orgs.mixed import MixedOrganization
from speceval.utils.logging import setup_logging


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate mixed provider models for compliance with the OpenAI Model Spec",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Caching functionality:
  This script supports multi-level caching to improve performance and save API costs:
   - Level "none": No caching (all API calls will be made each time)
   - Level "inputs": Only cache the challenging inputs generated by the AutoBencher
   - Level "generations": Cache both inputs and candidate model responses
   - Level "all": Cache inputs, candidate model responses, and judge model evaluations

  Cached data is stored in subdirectories under the cache-dir:
   - Inputs: {cache-dir}/autobencher_outputs/
   - Generations: {cache-dir}/generations/
   - Judgements: {cache-dir}/judgements/
""",
    )
    parser.add_argument(
        "--spec-path",
        type=str,
        default="data/specs/openai_model_spec.md",
        help="Path to the OpenAI Model Spec markdown file",
    )
    parser.add_argument(
        "--num-test-cases", type=int, default=5, help="Number of test cases to generate"
    )
    parser.add_argument(
        "--num-inputs-per-statement",
        type=int,
        default=1,
        help="Number of inputs to generate per statement",
    )
    parser.add_argument(
        "--test-all-statements", action="store_true", help="Test all statements instead of sampling"
    )
    parser.add_argument(
        "--output-path",
        type=str,
        default="compliance_report.html",
        help="Path to save the HTML report",
    )
    parser.add_argument(
        "--cache-level",
        type=str,
        choices=["none", "inputs", "generations", "all"],
        default="all",
        help="Level of caching to use (none, inputs, generations, all)",
    )
    parser.add_argument(
        "--cache-dir",
        type=str,
        default="data",
        help="Base directory to store cached data",
    )
    parser.add_argument(
        "--reports-dir",
        type=str,
        default="data/reports",
        help="Directory to store evaluation reports",
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    # API keys
    parser.add_argument(
        "--openai-api-key",
        type=str,
        default=None,
        help="OpenAI API key (alternatively, set OPENAI_API_KEY env var)",
    )
    parser.add_argument(
        "--anthropic-api-key",
        type=str,
        default=None,
        help="Anthropic API key (alternatively, set ANTHROPIC_API_KEY env var)",
    )
    parser.add_argument(
        "--together-api-key",
        type=str,
        default=None,
        help="Together API key (alternatively, set TOGETHER_API_KEY env var)",
    )

    # Candidate model configuration
    parser.add_argument(
        "--candidate-provider",
        type=str,
        choices=["openai", "anthropic", "together"],
        default="openai",
        help="Provider for candidate model",
    )
    parser.add_argument(
        "--candidate-model",
        type=str,
        default="gpt-4o-2024-08-06",
        help="Model name for the candidate",
    )

    # Evaluator model configuration
    parser.add_argument(
        "--evaluator-provider",
        type=str,
        choices=["openai", "anthropic"],
        default="openai",
        help="Provider for evaluator model",
    )
    parser.add_argument(
        "--evaluator-model",
        type=str,
        default="gpt-4o-2024-08-06",
        help="Model name for the evaluator",
    )

    # Judge model configuration
    parser.add_argument(
        "--judge-provider",
        type=str,
        choices=["openai", "anthropic"],
        default="openai",
        help="Provider for judge model",
    )
    parser.add_argument(
        "--judge-model",
        type=str,
        default="gpt-4o-2024-08-06",
        help="Model name for the judge",
    )

    args = parser.parse_args()

    # Configure logging
    logger = setup_logging(args.verbose, folder_name="eval_mixed_models")

    # Get API keys from args or environment variables
    openai_api_key = args.openai_api_key or os.environ.get("OPENAI_API_KEY")
    anthropic_api_key = args.anthropic_api_key or os.environ.get("ANTHROPIC_API_KEY")
    together_api_key = args.together_api_key or os.environ.get("TOGETHER_API_KEY")

    # Validate required API keys based on providers
    if (
        args.candidate_provider == "openai"
        or args.evaluator_provider == "openai"
        or args.judge_provider == "openai"
    ) and not openai_api_key:
        raise ValueError(
            "OpenAI API key is required when using OpenAI models. "
            "Either provide --openai-api-key or set OPENAI_API_KEY env var."
        )

    if (
        args.candidate_provider == "anthropic"
        or args.evaluator_provider == "anthropic"
        or args.judge_provider == "anthropic"
    ) and not anthropic_api_key:
        raise ValueError(
            "Anthropic API key is required when using Anthropic models. "
            "Either provide --anthropic-api-key or set ANTHROPIC_API_KEY env var."
        )

    if args.candidate_provider == "together" and not together_api_key:
        raise ValueError(
            "Together API key is required when using Together models. "
            "Either provide --together-api-key or set TOGETHER_API_KEY env var."
        )

    # Create the specification parser
    parser = MarkdownParser()

    # Load the specification
    spec_path = Path(args.spec_path)
    if not spec_path.exists():
        raise FileNotFoundError(f"Specification file not found: {spec_path}")

    spec = parser.from_file(spec_path)
    logger.info(f"Loaded specification with {len(spec.statements)} statements")

    # Display the first five statements if verbose mode is enabled
    if args.verbose and spec.statements:
        logger.debug("First five statements from the specification:")
        for i, statement in enumerate(spec.statements[:5]):
            logger.debug(f"{i+1}. {statement}")

    # Create the mixed organization
    organization = MixedOrganization(
        candidate_provider=args.candidate_provider,
        candidate_model_name=args.candidate_model,
        evaluator_provider=args.evaluator_provider,
        evaluator_model_name=args.evaluator_model,
        judge_provider=args.judge_provider,
        judge_model_name=args.judge_model,
        openai_api_key=openai_api_key,
        anthropic_api_key=anthropic_api_key,
        together_api_key=together_api_key,
    )

    # Map string cache level to enum
    cache_level_map = {
        "none": CacheLevel.NONE,
        "inputs": CacheLevel.INPUTS,
        "generations": CacheLevel.GENERATIONS,
        "all": CacheLevel.ALL,
    }
    cache_level = cache_level_map[args.cache_level]

    # Create the pipeline
    pipeline = StandardPipeline(
        specification=spec,
        organization=organization,
        num_test_cases=args.num_test_cases,
        statements_to_test=spec.statements if args.test_all_statements else None,
        num_inputs_per_statement=args.num_inputs_per_statement,
        cache_level=cache_level,
        cache_dir=args.cache_dir,
        verbose=args.verbose,
    )

    # Log organization configuration
    org_info = organization.get_info()
    logger.info("Evaluation configuration:")
    logger.info(f"  Candidate model: {org_info['candidate_model']}")
    logger.info(f"  Evaluator model: {org_info['evaluator_model']}")
    logger.info(f"  Judge model: {org_info['judge_model']}")

    # Run the evaluation
    logger.info(f"Running evaluation with {args.num_test_cases} test cases...")
    if args.test_all_statements:
        logger.info(
            f"Testing all {len(spec.statements)} statements with {args.num_inputs_per_statement} inputs per statement"
        )

    # Log caching configuration
    if cache_level == CacheLevel.NONE:
        logger.info("Caching is disabled")
    elif cache_level == CacheLevel.INPUTS:
        logger.info(f"Caching inputs only in {args.cache_dir}/autobencher_outputs")
    elif cache_level == CacheLevel.GENERATIONS:
        logger.info(f"Caching inputs and generations in {args.cache_dir}")
    else:  # CacheLevel.ALL
        logger.info(f"Caching inputs, generations, and judgements in {args.cache_dir}")

    results = pipeline.run()

    # Print a summary
    summary = results.get_summary()
    logger.info("\nEvaluation Summary:")
    logger.info(f"Total test cases: {summary['total_cases']}")
    logger.info(f"Compliant cases: {summary['compliant_cases']}")
    logger.info(f"Compliance rate: {summary['compliance_rate'] * 100:.2f}%")

    # Print compliance by statement type
    logger.info("\nCompliance by Statement Type:")
    for type_name, stats in summary["by_type"].items():
        logger.info(
            f"  {type_name}: {stats['compliance_rate'] * 100:.2f}% ({stats['compliant']}/{stats['total']})"
        )

    # Print compliance by authority level
    logger.info("\nCompliance by Authority Level:")
    for auth_name, stats in summary["by_authority"].items():
        logger.info(
            f"  {auth_name}: {stats['compliance_rate'] * 100:.2f}% ({stats['compliant']}/{stats['total']})"
        )

    # Generate HTML report
    output_path = Path(args.output_path)
    results.to_html(output_path)
    logger.info(f"\nGenerated HTML report: {output_path.absolute()}")

    # Also save JSON for further analysis
    json_path = output_path.with_suffix(".json")
    results.to_json(json_path)
    logger.info(f"Generated JSON report: {json_path.absolute()}")

    # Save reports to the specified directory with timestamp
    reports_dir = Path(args.reports_dir)
    if not reports_dir.exists():
        reports_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Created reports directory: {reports_dir.absolute()}")

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    model_name = organization.get_info()["candidate_model"].replace("/", "-")
    report_filename = f"{model_name}_evaluation_{timestamp}.json"
    report_path = reports_dir / report_filename

    results.to_json(report_path)
    logger.info(f"Saved detailed evaluation report to: {report_path.absolute()}")


if __name__ == "__main__":
    main()
