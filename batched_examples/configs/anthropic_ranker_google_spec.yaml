# Example configuration for run_batched_ranking.py

# List of paths to specification JSONL files
# These define the policy statements against which models are compared.
spec_name: google

spec_path: data/specs/google/jsonl/google.jsonl
  # - data/specs/anthropic/jsonl/anthropic_spec.jsonl
  # - data/specs/google/jsonl/google_play_spec.jsonl

# List of paths to directories containing candidate model generations.
# The basename of each directory will be used as the candidate model's name.
# Each directory should contain .json files named after statement_ids from the spec_files.
# Example: candidate_generation_dirs/model_A_results/some_statement_id.json
candidate_generation_dirs:
  - data/batched_generations/google/claude-3-5-haiku-20241022/20250509_025611/results
  - data/batched_generations/google/claude-3-5-sonnet-20240620/20250509_025521/results
  - data/batched_generations/google/claude-3-7-sonnet-20250219/20250509_025246/results
  - data/batched_generations/google/gpt-4.1-2025-04-14/20250509_030411/results
  - data/batched_generations/google/gpt-4.1-mini-2025-04-14/20250509_030821/results
  - data/batched_generations/google/gpt-4.1-nano-2025-04-14/20250509_133549/results
  - data/batched_generations/google/gpt-4o-2024-11-20/20250509_030502/results
  - data/batched_generations/google/gpt-4o-mini-2024-07-18/20250509_030631/results
  - data/converted_together_generations/google/deepseek-ai-DeepSeek-V3/20250509_054133/results
  - data/converted_together_generations/google/meta-llama-Llama-4-Maverick-17B-128E-Instruct-FP8/20250509_021252/results
  - data/converted_together_generations/google/meta-llama-Meta-Llama-3.1-405B-Instruct-Turbo/20250509_024643/results
  - data/converted_together_generations/google/Qwen-Qwen2-72B-Instruct/20250509_035308/results
  - data/converted_together_generations/google/Qwen-Qwen2.5-72B-Instruct-Turbo/20250509_080640/results
  - data/converted_together_generations/google/Qwen-Qwen3-235B-A22B-fp8-tput/20250509_054141/results
  - data/batched_google_generations/google/gemini-1.5-pro/20250509_181503/results
  - data/batched_google_generations/google/gemini-2.0-flash-001/20250509_181448/results


# Details for the ranking judge model (must be a batch-capable model)
ranking_judge_model_name: claude-3-7-sonnet-20250219
ranking_judge_org: anthropic # "openai" or "anthropic"

# Base directory to store all batch ranking metadata and final results
output_dir_base: data/batched_rankings

# Number of ranking prompts to include in each batch request to the judge model
# Adjust based on provider limits and desired throughput.
# OpenAI Batch API has a limit on requests per file (e.g., 50,000) and also per-minute rate limits for batch creation.
# Anthropic Batch API supports up to 100,000 requests per batch.
batch_size: 100000 #50000

# Optional temperature for the ranking judge model (0.0 for deterministic ranking)
temperature: 0.0

# Enable verbose logging for detailed output (true/false)
verbose: true

# API Keys (can also be set as environment variables)
# If set here, they will be used. Otherwise, the script checks environment variables.
# openai_api_key: YOUR_OPENAI_KEY_HERE
# anthropic_api_key: YOUR_ANTHROPIC_KEY_HERE
