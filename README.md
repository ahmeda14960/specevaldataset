# SpecEval

SpecEval is a framework for testing AI model compliance with various policy specifications. It uses adversarial testing techniques to generate inputs that might violate a specification, evaluates model responses, and provides comprehensive compliance reports.

## Architecture

The system consists of several key components:

1. **Specification Parser**: Extracts testable statements from policy documents
2. **AutoBencher**: Generates inputs designed to challenge compliance
3. **Candidate Models**: AI models being evaluated for compliance
4. **Evaluator Models**: Models that generate challenging inputs
5. **Judge Models**: Models that evaluate compliance of outputs

## Getting Started

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/speceval.git
cd speceval

# Create a micromamba environment with Python 3.11
micromamba create -n speceval python=3.11
micromamba activate speceval

# Install dependencies using uv
uv pip install -r requirements.txt

# Or install directly with uv (development mode)
uv pip install -e .
```

### Basic Usage

```python
from speceval import SpecificationParser, Pipeline
from speceval.organizations import OpenAI

# Load a specification
spec = SpecificationParser.from_file("specifications/openai_model_spec.md")

# Set up the evaluation pipeline
pipeline = Pipeline(
    specification=spec,
    organization=OpenAI(),
    num_test_cases=100,
)

# Run the evaluation
results = pipeline.run()

# Generate a report
results.to_html("compliance_report.html")
```

## Data Directory Structure

The framework uses a caching system to improve performance and save on API costs. By default, cached data is stored in the `data/` directory with the following structure:

```
data/
├── autobencher_outputs/         # Cache for challenging inputs
│   └── [model_identifier]/      # Subdirectories for specific models
│       └── [statement_id].json  # Cached inputs for a specific statement
│
├── generations/                 # Cache for model responses
│   └── [evaluator]x[candidate]/ # Subdirectories for model combinations
│       └── [statement_id].json  # Cached generations for a statement
│
├── judgements/                  # Cache for compliance evaluations
│   └── [evaluator]x[candidate]x[judge]/ # Model combination subdirectories
│       └── [statement_id].json  # Cached judgements for a statement
│
├── logs/                        # Logs directory (when verbose mode is enabled)
│   └── eval_openai_model_spec/  # Subdirectory for specific evaluation scripts
│       └── evaluation_*.log     # Timestamped log files
│
├── reports/                     # Evaluation reports
│   └── [model_name]_evaluation_*.json  # Timestamped evaluation reports
│
└── specs/                       # Specification files
    └── openai_model_spec.md     # OpenAI model specification
```

### Caching Levels

The framework supports multiple caching levels to control what data gets cached:

1. **None**: No caching (all API calls will be made each time)
2. **Inputs**: Only cache the challenging inputs generated by the AutoBencher
3. **Generations**: Cache both inputs and candidate model responses
4. **All**: Cache inputs, candidate model responses, and judge model evaluations

You can specify the cache level when running evaluations:

```bash
python -m examples.eval_openai_model_spec --cache-level generations --cache-dir data
```

Cache files are organized by model combinations to ensure that cached evaluations remain valid even when changing models or updating specifications.

## Project Structure

```
speceval/
├── speceval/
│   ├── __init__.py
│   ├── base/                # Abstract base classes
│   │   ├── __init__.py
│   │   ├── model.py         # Base model interfaces
│   │   ├── organization.py  # Organization interface
│   │   ├── parser.py        # Specification parser interface
│   │   ├── pipeline.py      # Pipeline interface
│   │   └── statement.py     # Statement representation
│   ├── models/              # Model implementations
│   │   ├── __init__.py
│   │   ├── openai.py        # OpenAI model wrappers
│   │   └── ...
│   ├── orgs/       # Organization implementations
│   │   ├── __init__.py
│   │   ├── openai.py        # OpenAI organization
│   │   └── ...
│   ├── parsers/             # Specification parser implementations
│   │   ├── __init__.py
│   │   ├── markdown.py      # Markdown parser
│   │   └── ...
│   ├── autobencher/         # AutoBencher implementation
│   │   ├── __init__.py
│   │   ├── generators.py    # Input generators
│   │   └── ...
│   └── utils/               # Utility functions
│       ├── __init__.py
│       └── ...
├── examples/                # Example usage
│   └── ...
├── specifications/          # Policy specifications
│   ├── openai_model_spec.md
│   └── ...
├── tests/                   # Unit tests
│   └── ...
├── README.md
├── requirements.txt
└── setup.py
```

## Extending the Framework

### Adding a New Organization

```python
from speceval.base import Organization, CandidateModel, EvaluatorModel, JudgeModel

class MyOrganization(Organization):
    def get_candidate_model(self) -> CandidateModel:
        return MyCandidateModel()

    def get_evaluator_model(self) -> EvaluatorModel:
        return MyEvaluatorModel()

    def get_judge_model(self) -> JudgeModel:
        return MyJudgeModel()
```

### Adding a New Specification Parser

```python
from speceval.base import SpecificationParser, Statement

class MyParser(SpecificationParser):
    def parse(self, content: str) -> list[Statement]:
        # Parse the content and return a list of statements
        statements = []
        # ...
        return statements
```

### Getting Started with the Framework

To begin, prepare the OpenAI Model Spec by running the following commands:
```bash
python -m examples.prepare_openai_spec --download
```

Next, execute the basic evaluation script:
```bash
python -m examples.eval_openai_model_spec --verbose
```

to run the together models
```bash
python -m examples.eval_together_models --test-all-statements --num-inputs-per-statement 5
```

for claude with gpt4 judge
```bash
python -m examples.eval_anthropic_models --test-all-statements --num-inputs-per-statement 4 --judge-provider openai --evaluator-provider openai --judge-model gpt-4o-mini-2024-07-18 --evaluator-model gpt-4o-2024-08-06
```

Below is running the openai spec but with pregenerated questions from the adaptive autobencher. Feel free to change it to any input directory as long as each root level folder is a json file with a name corresponding
to a statement in the relevant spec, with an "inputs" key.
```bash
python -m examples.eval_openai_model_spec --verbose --test-all-statements --num-inputs-per-statement 20  --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14
```

With google judge
```bash
python -m examples.eval_openai_model_spec --verbose --test-all-statements --num-inputs-per-statement 20  --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider google --judge-model gemini-2.0-flash-001
```

With anthropic judge

```bash
python -m examples.eval_openai_model_spec --verbose --test-all-statements --num-inputs-per-statement 20  --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider anthropic --judge-model claude-3-7-sonnet-20250219
```

```bash
python -m examples.eval_anthropic_models --test-all-statements --num-inputs-per-statement 20 --judge-provider openai --evaluator-provider openai --judge-model gpt-4.1-2025-04-14  --evaluator-model gpt-4.1-2025-04-14 --candidate-model claude-3-7-sonnet-20250219 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14
```

Google judge

```bash
python -m examples.eval_anthropic_models --test-all-statements --num-inputs-per-statement 20 --judge-provider google --evaluator-provider openai --judge-model gemini-2.0-flash-001  --evaluator-model gpt-4.1-2025-04-14 --candidate-model claude-3-7-sonnet-20250219 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14
```

Anthropic judge
```bash
python -m examples.eval_anthropic_models --test-all-statements --num-inputs-per-statement 20 --evaluator-provider openai  --evaluator-model gpt-4.1-2025-04-14 --candidate-model claude-3-7-sonnet-20250219 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider anthropic --judge-model claude-3-7-sonnet-20250219
```

```bash
python -m examples.eval_google_models --test-all-statements --num-inputs-per-statement 20 --judge-provider openai --evaluator-provider openai --judge-model gpt-4.1-2025-04-14  --evaluator-model gpt-4.1-2025-04-14 --candidate-model gemini-2.0-flash-001 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14
```


google judge on google
```bash
python -m examples.eval_google_models --test-all-statements --num-inputs-per-statement 20 --judge-provider openai --evaluator-provider openai --judge-model gpt-4.1-2025-04-14  --evaluator-model gpt-4.1-2025-04-14 --candidate-model gemini-2.0-flash-001 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider google --judge-model gemini-2.0-flash-001
```


claude judge on google
```bash
python -m examples.eval_google_models --test-all-statements --num-inputs-per-statement 20 --judge-provider openai --evaluator-provider openai --judge-model gpt-4.1-2025-04-14  --evaluator-model gpt-4.1-2025-04-14 --candidate-model gemini-2.0-flash-001 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider anthropic --judge-model claude-3-7-sonnet-20250219
```

```bash
python -m examples.eval_together_models --test-all-statements --num-inputs-per-statement 20 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14
```

google eval
```bash
python -m examples.eval_together_models --test-all-statements --num-inputs-per-statement 20 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider google --judge-model gemini-2.0-flash-001
```

together w anthropic eval
```bash
python -m examples.eval_together_models --test-all-statements --num-inputs-per-statement 20 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider anthropic --judge-model claude-3-7-sonnet-20250219
```

```bash
python -m examples.eval_deepseek_model --test-all-statements --num-inputs-per-statement 20 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14
```

google eval

```bash
python -m examples.eval_deepseek_model --test-all-statements --num-inputs-per-statement 20 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider google --judge-model gemini-2.0-flash-001
```

claude eval for deepseek

```bash
python -m examples.eval_deepseek_model --test-all-statements --num-inputs-per-statement 20 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider anthropic --judge-model claude-3-7-sonnet-20250219
```

```bash
python -m examples.eval_mistral_model --test-all-statements --num-inputs-per-statement 20 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14
```

mistral & google judge
```bash
python -m examples.eval_mistral_model --test-all-statements --num-inputs-per-statement 20 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider google --judge-model gemini-2.0-flash-001
```

mistral & anthropic judge
```bash
python -m examples.eval_mistral_model --test-all-statements --num-inputs-per-statement 20 --pregenerated-inputs-dir data/adaptive_autobencher_outputs/gpt-4.1-2025-04-14 --judge-provider anthropic --judge-model claude-3-7-sonnet-20250219
```

To evaluate model compliance on a likert scale [1-5] given paths of model generations run the following
```bash
# Example using OpenAI (default specified in config):
python score_likert_scale.py --config speceval/configs/likert_scoring_config.yaml --verbose

# Example overriding config to use Anthropic as judge:
python score_likert_scale.py --config speceval/configs/likert_scoring_config.yaml --judge-provider anthropic --verbose

# Example overriding config to use Google as judge (ensure GOOGLE_API_KEY or ADC is set):
python score_likert_scale.py --config speceval/configs/likert_scoring_config.yaml --judge-provider google --verbose
```

### Ranking Model Outputs

The framework includes a pipeline for pairwise ranking of model outputs to determine which model performs better for specific inputs according to a specification statement. This allows for comparative evaluation between different models.

#### How the Ranking Pipeline Works

1. **Input Structure**: The pipeline expects pre-generated model outputs organized in a specific directory structure with folders named as `{evaluator_model_name}x{candidate_model_name}`.

2. **Validation**: Before ranking, the pipeline validates that all model generations for each statement have consistent inputs.

3. **Pairwise Comparison**: For each pair of candidate models, a ranking model evaluates which output better aligns with the specification for each input.

4. **Score Format**: Ranking scores are: 1 (first model is better), -1 (second model is better), or 0 (tie).

5. **Cached Results**: Results are stored in JSON files for each statement and model pair.

#### Running the Ranking Pipeline

To generate rankings for model generations over the same dataset for each statement, run:

```bash
python examples/rank_outputs_openai.py --spec-path data/specs/openai/jsonl/openai.jsonl \
    --generations-dir data/generations/ \
    --output-dir data/rankings/ \
    --ranking-model-name gpt-4.1-mini-2025-04-14 \
    --verbose
```

```bash
python examples/rank_outputs_openai.py --spec-path data/specs/openai/jsonl/openai.jsonl --generations-dir data/generations/ --output-dir data/rankings/ --ranking-model-name gpt-4.1-mini-2025-04-14 --verbose --skip-existing --all-pairs
```

### Ranking Model Outputs in Batched Mode

To support scenarios where model generations are produced in batches (e.g., from automated evaluation runs) and stored with a different directory structure, the ranking pipeline includes a `--batched-generations-mode` flag.

**How Batched Mode Works:**

1.  **Input Directory Structure**: When using `--batched-generations-mode`, the `--generations-dir` argument should point to a provider-level directory. The expected structure under this directory is:
    ```
    {generations_dir}/  # e.g., data/batched_generations/openai/
    └── {model_name_A}/ # e.g., gemini-2.0-flash-001/
    │   ├── {run_id_1}/ # e.g., 20250507_005422/
    │   │   └── results/
    │   │       ├── {statement_id_1}.json
    │   │       ├── {statement_id_2}.json
    │   │       └── ...
    │   ├── {run_id_2}/
    │   │   └── results/
    │   │       └── ...
    │   └── ...
    └── {model_name_B}/
        ├── {run_id_X}/
        │   └── results/
        │       └── ...
        └── ...
    ```
    Each `{statement_id}.json` file within a `results` directory should be a JSON list of objects, where each object contains `"input_text"` and `"output_text"` keys.

2.  **Run Selection**: For each `{model_name}` directory, the pipeline automatically selects the "best" run to use for ranking. It does this by:
    *   Identifying all `{run_id}/results/` subdirectories.
    *   Counting the number of `.json` statement files within each `results` directory.
    *   Choosing the run with the **maximum number of statement files**.
    *   If there's a tie in the number of statement files, the run with the **lexicographically largest `run_id`** (implying most recent, assuming a timestamp-based naming convention like `YYYYMMDD_HHMMSS`) is selected.

3.  **Model Name Extraction**: The names of the candidate models (e.g., `gemini-2.0-flash-001`) are extracted from their respective directory names, potentially using the `extract_model_name_from_path` utility from `speceval.utils.parsing` for canonicalization.

4.  **Output Structure**: The output directory structure under `data/rankings/` remains the same as in the standard mode, using the provider name (from `{generations_dir}`) and the identified candidate model names.

**Example Command for Batched Mode:**

```bash
python examples/rank_outputs_openai.py --spec-path data/specs/openai/jsonl/openai.jsonl \
    --generations-dir data/batched_generations/openai/ \
    --output-dir data/rankings_sorta_batch/ \
    --ranking-model-name gemini-2.0-flash-001 \
    --batched-generations-mode \
    --ranking-org google \
    --all-pairs \
    --verbose
```

**Key considerations for Batched Mode:**
*   Ensure your batched generations directory follows the specified structure.
*   The JSON files within the `results` directory must conform to the list of `{"input_text": ..., "output_text": ...}` objects.
*   The pipeline relies on the `run_id` naming to break ties by recency if statement counts are equal.

### new run for anthropic!
```bash
python examples/rank_outputs_openai.py --spec-path data/specs/openai/jsonl/openai.jsonl --generations-dir data/generations/ --output-dir data/rankings/ --ranking-org anthropic --verbose --skip-existing --all-pairs
```

# ranker run for gemini
```bash
python examples/rank_outputs_openai.py --spec-path data/specs/openai/jsonl/openai.jsonl --generations-dir data/generations/ --output-dir data/rankings/ --ranking-org google --verbose --skip-existing --all-pairs
```

**Key considerations:**
- Ensure your generations directory follows the expected structure
- Make sure all models have generated outputs for the same inputs
- The ranking model should be capable of making nuanced judgments about compliance
- The process can be API-intensive for large datasets with many model pairs

**Output directory structure:**
```
data/rankings/
└── {evaluator_model_name}/
    ├── {candidate_model_A}VS{candidate_model_B}/
    │   ├── {statement_id_1}.json
    │   ├── {statement_id_2}.json
    │   └── ...
    ├── {candidate_model_A}VS{candidate_model_C}/
    │   └── ...
    └── ...
```

### Single-Spec Ranking via YAML

The script `examples/rank_outputs_singleton.py` reads a YAML configuration file and runs the standard (synchronous) `RankingPipeline` to rank pre-generated model outputs for one specification.

**Install dependencies if needed:**
```bash
pip install pyyaml
```

**Example usage:**
```bash
python examples/rank_outputs_singleton.py \
  --config-file scripts/configs/rank_outputs_singleton_config.yaml
```

**Configuration schema (`scripts/configs/rank_outputs_singleton_config.yaml`):**
```yaml
# Path to specification JSONL file
spec_path: "data/specs/openai/jsonl/openai.jsonl"

# List of candidate output directories, each containing <statement_id>.json files
candidate_generation_dirs:
  - "data/generations/gpt-4.1-2025-04-14xclaude-3-7-sonnet-20250219"
  - "data/generations/gpt-4.1-2025-04-14xgemini-2.0-flash-001"

# Name of the evaluator (input generator) model (used for folder naming)
evaluator_model_name: "gpt-4.1-2025-04-14"

# Output settings
output_dir_base: "data/rankings"

# Ranking model provider and name
ranking_org: "openai"
ranking_model_name: "gpt-4.1-mini-2025-04-14"

# Logging and pairing options
verbose: true
all_pairs: true          # Compare A vs B and B vs A
skip_existing: true      # Skip already-ranked statement/pair
batched_generations_mode: false  # Not used for candidate dirs mode

# API Keys
openai_api_key: "YOUR_OPENAI_API_KEY"
# anthropic_api_key: "YOUR_ANTHROPIC_API_KEY"
```

**Output directory structure:**
```
data/rankings/
└── {RANKER_MODEL_NAME}/
    └── {EVALUATOR_MODEL_NAME}/
        └── {modelA}x{modelB}/
            ├── {statement_id_1}.json
            └── ...
```

### TODO
Add a hotfix for position bias


This framework is designed to be extensible for other organizations by implementing the necessary interfaces. This makes it a versatile tool for assessing AI model compliance with diverse policy specifications.

## Development Workflow

### Using pre-commit

This project uses pre-commit to ensure code quality and consistency. Pre-commit runs various hooks (including black and flake8) before each commit.

```bash
# Install pre-commit hooks
pre-commit install

# Run pre-commit on all files
pre-commit run --all-files

# The hooks will also run automatically on git commit
```

The pre-commit configuration includes:
- Black for code formatting
- Flake8 for linting
- Basic file checks (trailing whitespace, YAML validation, etc.)

## License

This project is licensed under the MIT License - see the LICENSE file for details.


You’re absolutely right—the flip isn’t coming from the compliance side at all but from the ranking pipeline.  In
```python
# … inside _process_completed_ranking_batch() …
output_data_for_file = {
    "metadata": { … },
    "rankings": sorted(
        rankings_list,
        key=lambda x: x.get("input", "")
    ),  # <<<< here’s the culprit
}
```
we forcibly alphabetize your entire results array by the `"input"` string before dumping the JSON.

Because earlier on we built `rankings_list` by iterating a Python `set` of common inputs (which is itself unordered) and then we sort that “unordered” list alphabetically, the final output’s order is not the same as your original generation order—it’s purely sorted by text.  In practice that often looks “flipped” relative to what you expected.

To preserve the original sequence you saw in your generation files:

1. Remove (or change) that `sorted(…, key=…)` call so you simply emit
   ```python
   "rankings": rankings_list
   ```
   instead of an alphabetically sorted list.

2. (Optional) If you really need a deterministic sort, sort by an explicit index field—e.g. capture the original enumeration index when you build each task and then sort on that integer—rather than sorting by the raw input text.

Making that one-line change will stop your JSON from getting reordered behind the scenes.
